{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medblocks/youtube-transcript/blob/main/WhisperYouTube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're looking at this on GitHub and new to Python Notebooks or Colab, click the Google Colab badge above üëÜ\n",
        "\n",
        "\n",
        "#**Creating YouTube transcripts with OpenAI's Whisper model**\n",
        "\n",
        "üì∫ Getting started video: https://youtu.be/kENRf82_RQs\n",
        "\n",
        "*Colab beginner notes:*\n",
        "<br>\n",
        "1. These files are being loaded on a virtual machine in the cloud. Nothing is being downloaded to your computer (except for the transcript when you click to download it.) When you close this session the instance will be erased.\n",
        "<br>\n",
        "2. The run button is visible when you move your mouse close to the left edge of the code block. It looks kind of like this: ‚ñ∂Ô∏è ...but round...and white on black...so nothing like this. You'll know it when you see it.\n",
        "\n",
        "###**Note: For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n",
        "\n",
        "\n",
        "**Step 1.** Follow the instructions in each block and select the options you want\n",
        "<br>\n",
        "**Step 2.** Get the url of the video you want to transcribe\n",
        "<br>\n",
        "**Step 3.** Refresh the folder on the left and download your transcript\n",
        "<br>\n",
        "**Step 4.** Go to your YouTube account and upload the transcript to the video it came from and use \"autosync.\"\n",
        "\n",
        "That's it!\n",
        "\n",
        "Have a question? Hit me up on Twitter:[ @AndrewMayne](https://twitter.com/andrewmayne)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What is this?**\n",
        "<br>\n",
        "This is a Python notebook that creates a transcript from a YouTube url using OpenAI's Whisper transcription model that you can then upload to YouTube using the autosync feature to create captions.\n",
        "<br>  \n",
        "**What is OpenAI's Whisper model?**\n",
        "<br>\n",
        "Whisper is an automatic speech recognition (ASR) neural net created by OpenAI that transcribes audio at close to human level.\n",
        "<br>\n",
        "<br>\n",
        "**Why use this?**\n",
        "<br>\n",
        "The quality of the OpenAI Whisper model is amazing (I am slightly biased, but seriously, check it out.) You can also use it to transcribe in other languages.\n",
        "<br>\n",
        "<br>\n",
        "**What do the different model sizes do?**\n",
        "<br>\n",
        "Each model size has an improvement in quality ‚Äì especially with different languages. I've found that for a YouTube video with clear speech, the base model works really well. If you see transcription errors, you can try a larger model.\n",
        "<br>\n",
        "<br>\n",
        "**Do I need timestamps?**\n",
        "<br>\n",
        "Nope. YouTube's autosync function will match the text to the spoken words and syncs up really well. All you need is each spoken sentence in a .txt file.\n",
        "<br>\n",
        "<br>\n",
        "**How do I do this?**\n",
        "<br>\n",
        "Just follow each step. If you've never used Colab of a Python notebook, don't panic. It's super easy and runs in the cloud.\n",
        "<br>\n",
        "<br>\n",
        "**Does this cost anything to use?**\n",
        "<br>\n",
        "Nope. You can use Colab for free and Whisper is an open source model.\n",
        "<br>\n",
        "<br>\n",
        "[Tips for creating a YouTube transcript file](https://support.google.com/youtube/answer/2734799?hl=en)\n",
        "<br>\n",
        "[Information on OpenAI's Whisper model](https://openai.com/blog/whisper/)\n",
        "<br>\n",
        "[OpenAI's Whisper GitHub page](https://github.com/openai/whisper)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvz5JoKjwKAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Click the start button in the upper left side of this block to load the necessary libraries\n",
        "\n",
        "You will need to run this every time you reload this notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install yt-dlp\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import re\n",
        "import yt_dlp"
      ],
      "metadata": {
        "id": "j6svgIwL1a-J",
        "outputId": "348ede01-5a72-4ade-efb1-9df4d0b264be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.12.13-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/172.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.12.13-py3-none-any.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2024.12.13\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-24n8b1ub\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-24n8b1ub\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. Select the model you want to use.\n",
        "\n",
        "Base works really well so it's the default.\n",
        "\n",
        "(For multilingual, remove \".en\" from the model name.)\n",
        "\n",
        "Click the run button after you've made your choice (or left it at default.)\n",
        "\"\"\"\n",
        "\n",
        "# model = whisper.load_model(\"tiny.en\")\n",
        "# model = whisper.load_model(\"base.en\")\n",
        "# model = whisper.load_model(\"small.en\")\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "9oRA4UIe104O",
        "outputId": "930781e9-2ae3-49e4-a0ec-329cf9d93a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:46<00:00, 66.5MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3. Click the run button and input your YouTube URL in the box below then click enter.\n",
        "You can use this one to test: https://www.youtube.com/watch?v=CnT-Na1IeVI\n",
        "The video will be loaded and the audio extracted (this is usually the longest part of the process.)\n",
        "Your transcript will appear in the folder on the left (you may have to refresh the folder to see it.)\n",
        "You can download the file when it's completed and upload it on your video's detail page using \"autosync.\"\n",
        "\"\"\"\n",
        "# This will prompt you for a YouTube video URL\n",
        "url = input(\"Enter a YouTube video URL: \")\n",
        "\n",
        "# Create a yt-dlp options dictionary\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    'outtmpl': '%(title)s.%(ext)s',\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Download the video and extract the audio\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        file_path = ydl.prepare_filename(info)\n",
        "        file_path = file_path.replace('.webm', '.mp3')\n",
        "        file_path = file_path.replace('.m4a', '.mp3')\n",
        "\n",
        "    # Get the duration\n",
        "    duration = librosa.get_duration(filename=file_path)\n",
        "    start = time.time()\n",
        "\n",
        "    # Set Whisper to return word timestamps\n",
        "    result = model.transcribe(file_path, word_timestamps=True)\n",
        "\n",
        "    end = time.time()\n",
        "    seconds = end - start\n",
        "    print(\"Video length:\", duration, \"seconds\")\n",
        "    print(\"Transcription time:\", seconds)\n",
        "\n",
        "    # Process segments with timestamps\n",
        "    formatted_segments = []\n",
        "    for segment in result[\"segments\"]:\n",
        "        # Convert start time to MM:SS format\n",
        "        start_time = int(segment[\"start\"])\n",
        "        minutes = start_time // 60\n",
        "        seconds = start_time % 60\n",
        "        timestamp = f\"**{minutes}:{seconds:02d}**\"\n",
        "\n",
        "        # Add timestamp and text\n",
        "        formatted_text = f\"{timestamp} {segment['text'].strip()}\"\n",
        "        formatted_segments.append(formatted_text)\n",
        "\n",
        "    # Join segments with double newlines\n",
        "    text = \"\\n\\n\".join(formatted_segments)\n",
        "\n",
        "    # Print segments\n",
        "    for segment in formatted_segments:\n",
        "        print(segment)\n",
        "\n",
        "    # Save the file as .txt\n",
        "    name = file_path + \".txt\"\n",
        "    with open(name, \"w\", encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "    print(\"\\n\\n\", \"-\"*100, \"\\n\\nYour transcript is here:\", name)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "JmbHC2-S33Kl",
        "outputId": "3044ac5d-a537-4aac-f3dc-8956a0e63bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a YouTube video URL: https://www.youtube.com/watch?v=ztI5OTOoLJE\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=ztI5OTOoLJE\n",
            "[youtube] ztI5OTOoLJE: Downloading webpage\n",
            "[youtube] ztI5OTOoLJE: Downloading ios player API JSON\n",
            "[youtube] ztI5OTOoLJE: Downloading mweb player API JSON\n",
            "[youtube] ztI5OTOoLJE: Downloading m3u8 information\n",
            "[info] ztI5OTOoLJE: Downloading 1 format(s): 251\n",
            "[download] Destination: Breakthroughs in Health Data PlatformsÔºö openEHR, FHIR & Distributed SQL ÔΩú DHH Podcast Ep. 12.webm\n",
            "[download] 100% of   81.23MiB in 00:00:21 at 3.72MiB/s   \n",
            "[ExtractAudio] Destination: Breakthroughs in Health Data PlatformsÔºö openEHR, FHIR & Distributed SQL ÔΩú DHH Podcast Ep. 12.mp3\n",
            "Deleting original file Breakthroughs in Health Data PlatformsÔºö openEHR, FHIR & Distributed SQL ÔΩú DHH Podcast Ep. 12.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e3a71fc0eb3d>:31: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
            "\tThis alias will be removed in version 1.0.\n",
            "  duration = librosa.get_duration(filename=file_path)\n"
          ]
        }
      ]
    }
  ]
}